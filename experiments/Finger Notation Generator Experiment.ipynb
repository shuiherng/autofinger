{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Data Pipeline:\n",
    "Raw Data in Directory --> Extract Semantics Data --> Get Encoding --> Collect into batches --> Yield labels from expert system --> Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import constraint\n",
    "import logging\n",
    "\n",
    "from itertools import chain\n",
    "from string import digits\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "import finger_oracle\n",
    "# Technically no need to import semantics_generator for this experiment,\n",
    "# Can simply use semantics file as ground truth\n",
    "# sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "# import semantics_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = ['../data/package_aa', '../data/package_ab']\n",
    "DONE_STATUS_OUTPUT = '../tmp/finger_notation_generator_output.pkl'\n",
    "IGNORE_LIST_PATH = '../tmp/finger_notation_generator_ignore.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Notes into black/non-black key encoding and semitone distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static variables for encoding inputs into black/non-black keys and semitone distances\n",
    "BLACK_KEYS = set(['C#','Bb','D#','Eb','F#','Gb','G#','Ab','A#','Bb'])\n",
    "KEY_MAP = {'Cb': -1, 'C': 0, 'C#': 1, 'Db': 1, 'D': 2, 'D#': 3, 'Eb': 3,\n",
    "           'E': 4, 'Fb': 4, 'E#': 5, 'F': 5, 'F#': 6, 'Gb': 6, 'G': 7, 'G#': 8,\n",
    "           'Ab': 8, 'A': 9, 'A#': 10, 'Bb': 10, 'B': 11, 'B#': 12}\n",
    "REMOVE_DIGITS = str.maketrans('', '', digits)\n",
    "OCTAVE_SEMITONES = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enumerate black keys. Do not filter by checking for '#' or 'b' in the string, because\n",
    "# composers sometimes write things like E# or Cb, which are not black keys.\n",
    "# The string translate() method is faster than iterating manually.\n",
    "def is_black_key(key):\n",
    "    key = key.translate(REMOVE_DIGITS)\n",
    "    return key in BLACK_KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get semitone distance\n",
    "# if no octave digit indicators in input, assume all to be the same\n",
    "# octave 4 was arbitrarily picked as C4 is middle C.\n",
    "def get_semitone_distances(notes):\n",
    "    # fill in missing octave digits\n",
    "    notes = [x for x in map(lambda n: n if any(i.isdigit() for i in n) else n+'4', notes)]\n",
    "    \n",
    "    # compute semitone differences\n",
    "    # initialize first note semitone distance to 0\n",
    "    notes = [notes[0]] + notes\n",
    "    diffs = []\n",
    "    for i in range(len(notes) - 1):\n",
    "        before = notes[i]\n",
    "        after = notes[i+1]\n",
    "        \n",
    "        before = int(before[-1]) * OCTAVE_SEMITONES + KEY_MAP[before[:-1]]\n",
    "        after = int(after[-1]) * OCTAVE_SEMITONES + KEY_MAP[after[:-1]]\n",
    "        diffs.append(after - before)\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes tokens as inputs to model\n",
    "# for the piano model, we don't really care which octave we're at\n",
    "# in fact, we can simplify the notes into semitone distance from previous note,\n",
    "# and whether the current note is a white or black key (affects ease of playing)\n",
    "# note that the primus dataset does not have double-sharp/flat as inputs\n",
    "def get_encoding(tokens):\n",
    "    # combine multiple lines, if any\n",
    "    tokens = '\\t'.join(tokens)\n",
    "    tokens = tokens.split('\\t')\n",
    "    # only use notes\n",
    "    tokens = [y for y in filter(lambda x: x.startswith('note-') or x.startswith('gracenote-'), tokens)]\n",
    "    # ignore note lengths\n",
    "    tokens = [y for y in map(lambda x: x.split('_')[0][5:] if x.startswith('note-') else x.split('_')[0][10:]\n",
    "                             , tokens)]\n",
    "    # get black_keys\n",
    "    black_keys = [x for x in map(is_black_key, tokens)]\n",
    "    semitone_distances = get_semitone_distances(tokens)\n",
    "    \n",
    "    return [(x,y) for x, y in zip(black_keys, semitone_distances)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heuristic-based labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treats this as a constraint satisfaction problem, with heuristics as strict constraints\n",
    "# An alternative neighbourhood-search algorithm exists at https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/11801/Herremans%20A%20variable%20neighborhood%20search%20algorithm%202015%20Accepted.pdf\n",
    "# but it is over-engineered for this purpose\n",
    "def solve(enc):\n",
    "    variables = [i for i in range(len(enc))]\n",
    "    is_black_key = [e[0] for e in enc]\n",
    "    distance = [e[1] for e in enc]\n",
    "    problem = constraint.Problem()\n",
    "    \n",
    "    oracles = []\n",
    "    for i in range(len(enc) - 1):\n",
    "        ibk_first, ibk_second = enc[i][0], enc[i+1][0]\n",
    "        d = enc[i+1][1]\n",
    "        oracles.append(finger_oracle.FingerOracle(ibk_first, ibk_second, d))\n",
    "    for v in variables:\n",
    "        problem.addVariable(v, range(1,6))\n",
    "    \n",
    "    # consecutive notes must not be played with the same finger\n",
    "    for i in range(0, len(variables) - 1):\n",
    "        problem.addConstraint(constraint.AllDifferentConstraint(), [i, i + 1])\n",
    "    \n",
    "    for i in range(len(enc) - 1):\n",
    "        problem.addConstraint(constraint.FunctionConstraint(oracles[i].is_valid)\n",
    "                              , [i, i + 1])\n",
    "    \n",
    "    # can use problem.getSolutionIter() here to generate multiple solutions\n",
    "    \n",
    "    solution = problem.getSolution()\n",
    "    if not solution:\n",
    "        return None\n",
    "    \n",
    "    output = []\n",
    "    for i in range(len(solution)):\n",
    "        output.append(solution[i])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull data from data directory/directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get semantic data from dataset\n",
    "# assumes data path is data_src[i]/folder_name/folder_name.semantic, in-line with primus dataset\n",
    "def get_semantics_data(data_src, done, ignore):\n",
    "    for src in data_src:\n",
    "        for i, folder in enumerate(next(os.walk(src))[1]):\n",
    "            path_to_file = os.path.join(src, folder, \"{}.semantic\".format(folder))\n",
    "            if path_to_file in done or path_to_file in ignore:\n",
    "                continue\n",
    "            with open(path_to_file, \"r\") as f:\n",
    "                try:\n",
    "                    yield path_to_file, folder, f.readlines()\n",
    "                except FileNotFoundError:\n",
    "                    logging.warn(\"Data file not found: {}\".format(path_to_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    done = set()\n",
    "    ignore = set()\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # check for existence of files\n",
    "    if not os.path.exists(DONE_STATUS_OUTPUT):\n",
    "        with open(DONE_STATUS_OUTPUT, 'wb+') as f:\n",
    "            pass\n",
    "    if not os.path.exists(IGNORE_LIST_PATH):\n",
    "        with open(IGNORE_LIST_PATH, 'w+') as f:\n",
    "            pass\n",
    "    \n",
    "    # load file which are already done and ignore list\n",
    "    with open(DONE_STATUS_OUTPUT, 'rb') as f:\n",
    "        try:\n",
    "            done = pkl.load(f)\n",
    "        except EOFError:\n",
    "            done = set()\n",
    "    with open(IGNORE_LIST_PATH, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                ignore.add(line)\n",
    "    \n",
    "    last_file = \"No File\"\n",
    "    try:\n",
    "        with tqdm() as pbar:\n",
    "            pbar.set_description(\"Generating finger positions\")\n",
    "            for i, (path_to_file, filename, semantics_data) in enumerate(get_semantics_data(DATA_DIR, done, ignore)):\n",
    "                last_file = path_to_file\n",
    "                finger_file_path = \"{}.finger\".format(path_to_file[:-9])\n",
    "                encoding = get_encoding(semantics_data)\n",
    "                pseudo_labels = solve(encoding)\n",
    "                if pseudo_labels:\n",
    "                    pseudo_labels = map(str, pseudo_labels)\n",
    "                    with open(finger_file_path, \"w\") as f:\n",
    "                        f.write(' '.join(map(str, pseudo_labels)))\n",
    "                        done.add(path_to_file)\n",
    "                    pbar.update(1)\n",
    "                else:\n",
    "                    with open(IGNORE_LIST_PATH, 'a') as f:\n",
    "                        f.write('{}\\n'.format(path_to_file))\n",
    "                    \n",
    "                # pprint(semantics_data)\n",
    "                # pprint(pseudo_labels)\n",
    "                # pprint(encoding)\n",
    "                with open(DONE_STATUS_OUTPUT, 'wb') as f:\n",
    "                    pkl.dump(done, f)\n",
    "    except:\n",
    "        logging.error(\"Exception triggered while processing {}\".format(last_file))\n",
    "        pbar.close()\n",
    "        raise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4066da902848b88208f832c5c96d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
